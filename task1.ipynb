{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb24a1d7",
   "metadata": {},
   "source": [
    "# Task 1: Quora Insincere Questions Classification\n",
    "Fine-tuning DeBERTa-v3-base for binary classification of toxic/insincere questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2cfe65-5f63-4ed7-8303-c6add847632f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.12.11 environment at: /home/zeus/miniconda3/envs/cloudspace\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m2 packages\u001b[0m \u001b[2min 9ms\u001b[0m\u001b[0m\n",
      "\u001b[2mUsing Python 3.12.11 environment at: /home/zeus/miniconda3/envs/cloudspace\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m39 packages\u001b[0m \u001b[2min 269ms\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 68ms\u001b[0m\u001b[0m                                               \n",
      "\u001b[2K░░░░░░░░░░░░░░░░░░░░ [0/1] \u001b[2mInstalling wheels...                                 \u001b[0m\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mFailed to hardlink files; falling back to full copy. This may lead to degraded performance.\n",
      "         If the cache and target directories are on different filesystems, hardlinking may not be supported.\n",
      "         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 25ms\u001b[0m\u001b[0m                                 \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1maccelerate\u001b[0m\u001b[2m==1.12.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install transformers datasets\n",
    "!uv pip install 'accelerate>=0.26.0'\n",
    "!uv pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909f00c5-04f7-4592-9b52-a22951db6c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip  \"Language Challenge.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91231775",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import torch\n",
    "from transformers import Trainer\n",
    "from torch import nn\n",
    "from datasets import ClassLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e90a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\n",
    "    \"Language Challenge/quora-insincere-questions-classification/train.csv\"\n",
    ")\n",
    "test = pd.read_csv(\n",
    "    \"Language Challenge/quora-insincere-questions-classification/test.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ead79bc",
   "metadata": {},
   "source": [
    "## Data Preprocessing & Tokenization\n",
    "Compute class weights for imbalanced data and tokenize using DeBERTa tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6d02a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: tensor([0.5330, 8.0814])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:566: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7090cb45eb6445583abcc7a25802f66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1306122 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "130091dcc5664e54aed2a779fa3b857e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/1306122 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_weights = compute_class_weight(\n",
    "    \"balanced\", classes=np.array([0, 1]), y=train[\"target\"].values\n",
    ")\n",
    "\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "print(f\"Class weights: {class_weights}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"question_text\"], truncation=True, padding=False, max_length=256\n",
    "    )\n",
    "\n",
    "\n",
    "dataset = Dataset.from_pandas(train[[\"question_text\", \"target\"]])\n",
    "dataset = dataset.map(tokenize_function, batched=True)\n",
    "dataset = dataset.rename_column(\"target\", \"labels\")\n",
    "dataset = dataset.cast_column(\"labels\", ClassLabel(names=[\"non_toxic\", \"toxic\"]))\n",
    "\n",
    "split_dataset = dataset.train_test_split(\n",
    "    test_size=0.1, stratify_by_column=\"labels\", seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d77580",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "Custom WeightedTrainer with class-weighted cross-entropy loss to handle class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11c958d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_231217/1282398524.py:52: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedTrainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='36736' max='36736' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [36736/36736 1:50:29, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.261422</td>\n",
       "      <td>0.696927</td>\n",
       "      <td>0.836375</td>\n",
       "      <td>0.599483</td>\n",
       "      <td>0.832199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.223000</td>\n",
       "      <td>0.220058</td>\n",
       "      <td>0.662574</td>\n",
       "      <td>0.815779</td>\n",
       "      <td>0.523762</td>\n",
       "      <td>0.901497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.241000</td>\n",
       "      <td>0.246188</td>\n",
       "      <td>0.707477</td>\n",
       "      <td>0.841862</td>\n",
       "      <td>0.601491</td>\n",
       "      <td>0.858805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.169200</td>\n",
       "      <td>0.255871</td>\n",
       "      <td>0.721434</td>\n",
       "      <td>0.849776</td>\n",
       "      <td>0.627044</td>\n",
       "      <td>0.849276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.166900</td>\n",
       "      <td>0.250423</td>\n",
       "      <td>0.714278</td>\n",
       "      <td>0.845477</td>\n",
       "      <td>0.605209</td>\n",
       "      <td>0.871303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.158500</td>\n",
       "      <td>0.267364</td>\n",
       "      <td>0.728048</td>\n",
       "      <td>0.853367</td>\n",
       "      <td>0.633804</td>\n",
       "      <td>0.855216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.185000</td>\n",
       "      <td>0.232511</td>\n",
       "      <td>0.721335</td>\n",
       "      <td>0.849405</td>\n",
       "      <td>0.615103</td>\n",
       "      <td>0.871922</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=36736, training_loss=0.2102616144026198, metrics={'train_runtime': 6630.3565, 'train_samples_per_second': 354.584, 'train_steps_per_second': 5.541, 'total_flos': 5.903708433069824e+16, 'train_loss': 0.2102616144026198, 'epoch': 2.0})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(\n",
    "        self, model, inputs, return_outputs=False, num_items_in_batch=None\n",
    "    ):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=self.class_weights)\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"microsoft/deberta-v3-base\", num_labels=2\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./deberta-tuned\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=5000,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=5000,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    logging_steps=100,\n",
    "    warmup_steps=1000,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\n",
    "        \"f1\": f1_score(labels, predictions, average=\"binary\"),\n",
    "        \"f1_macro\": f1_score(labels, predictions, average=\"macro\"),\n",
    "        \"precision\": precision_score(labels, predictions),\n",
    "        \"recall\": recall_score(labels, predictions),\n",
    "    }\n",
    "\n",
    "\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=split_dataset[\"train\"],\n",
    "    eval_dataset=split_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.class_weights = class_weights.to(trainer.args.device)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e53334f",
   "metadata": {},
   "source": [
    "## Inference\n",
    "Load trained model and run predictions on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06369b8e-62a3-47c0-b68a-b6d258756185",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from 'deberta-tuned/checkpoint-30000' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OptimizedModule(\n",
       "  (_orig_mod): DebertaV2ForSequenceClassification(\n",
       "    (deberta): DebertaV2Model(\n",
       "      (embeddings): DebertaV2Embeddings(\n",
       "        (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): DebertaV2Encoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x DebertaV2Layer(\n",
       "            (attention): DebertaV2Attention(\n",
       "              (self): DisentangledSelfAttention(\n",
       "                (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (pos_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): DebertaV2SelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): DebertaV2Intermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): DebertaV2Output(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (rel_embeddings): Embedding(512, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (pooler): ContextPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "    (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_DIR = \"deberta-task1\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_DIR)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = model.to(device)\n",
    "model = torch.compile(model)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12cc3fb-db80-49a9-acb1-7dafd843f0a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f303617e6d646069bfdc4c58f184883",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1306122 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5103 [00:00<?, ?it/s]W0113 06:23:50.074000 10671 /system/conda/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/_inductor/utils.py:1436] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "100%|██████████| 5103/5103 [16:20<00:00,  5.20it/s]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "\n",
    "test_ds = Dataset.from_pandas(train[[\"question_text\"]])\n",
    "\n",
    "\n",
    "def tok_fn(batch):\n",
    "    return tokenizer(batch[\"question_text\"], truncation=True, max_length=256)\n",
    "\n",
    "\n",
    "test_ds = test_ds.map(tok_fn, batched=True, remove_columns=[\"question_text\"])\n",
    "\n",
    "collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "loader = DataLoader(test_ds, batch_size=256, shuffle=False, collate_fn=collator)\n",
    "\n",
    "all_logits = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(loader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        logits = model(**batch).logits\n",
    "        all_logits.append(logits.cpu())\n",
    "\n",
    "logits = torch.cat(all_logits, dim=0).numpy()\n",
    "probs = torch.softmax(torch.tensor(logits), dim=1).numpy()\n",
    "pred_label = np.argmax(logits, axis=1)\n",
    "p_toxic = probs[:, 1]\n",
    "\n",
    "train[\"pred_label\"] = pred_label\n",
    "train[\"p_toxic\"] = p_toxic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fada32e1-aca7-47a3-a228-5d0471efa073",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = train[[\"qid\", \"pred_label\"]].copy()\n",
    "submission.columns = [\"qid\", \"prediction\"]\n",
    "submission.to_csv(\"task1_pred.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5092ac",
   "metadata": {},
   "source": [
    "## Evaluation & Threshold Tuning\n",
    "Evaluate model performance and find optimal classification threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f6189a-0565-4f2b-bfcf-71da5b65ee84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.6842662383881153\n",
      "Recall: 0.9279297116693479\n",
      "F1: 0.7876846152230089\n"
     ]
    }
   ],
   "source": [
    "y_true = train[\"target\"].values\n",
    "y_pred = train[\"pred_label\"].values\n",
    "\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495df05a-96d7-465b-b0ab-9fb446c8b12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.8581632653061224\n",
      "Best F1: 0.8083354742941965\n",
      "Best Precision: 0.7503524299114951\n",
      "Best Recall: 0.8760301942828858\n"
     ]
    }
   ],
   "source": [
    "y_true = train[\"target\"].values\n",
    "probs = train[\"p_toxic\"].values\n",
    "\n",
    "best_f1 = 0\n",
    "best_t = 0\n",
    "best_precision = 0\n",
    "best_recall = 0\n",
    "for t in np.linspace(0.05, 0.95, 50):\n",
    "    f1 = f1_score(y_true, probs > t)\n",
    "    precision = precision_score(y_true, probs > t)\n",
    "    recall = recall_score(y_true, probs > t)\n",
    "    if f1 > best_f1:\n",
    "        best_t = t\n",
    "        best_f1 = f1\n",
    "        best_precision = precision\n",
    "        best_recall = recall\n",
    "\n",
    "print(\"Best threshold:\", best_t)\n",
    "print(\"Best F1:\", best_f1)\n",
    "print(\"Best Precision:\", best_precision)\n",
    "print(\"Best Recall:\", best_recall)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
